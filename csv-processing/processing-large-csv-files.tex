\documentclass[11pt]{article}
\usepackage[fntef,hyperref,UTF8]{ctexcap}
\usepackage[a4paper, margin=2cm]{geometry}
\usepackage{fancyhdr,lastpage}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{extarrows}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{listings}
\lstset{
extendedchars=false,
language=python,
basicstyle=\ttfamily,
keywordstyle=\bf,
showstringspaces=false,
tabsize=2,
numbers=left,
numbersep=5pt,
frame=single,
framexleftmargin=0.8cm,
xleftmargin=0.8cm
}

\pagestyle{fancy}
\fancyhf{}
\rfoot{\scriptsize{\kaishu 第\thepage 页, 共\pageref{LastPage}页}}
\renewcommand\headrulewidth{0pt} % Removes funny header line
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries\flushleft} {{\bfseries\thesection\space}}{8pt}{}

\renewcommand{\theenumii}{\arabic{enumii}}
\renewcommand{\labelenumii}{(\theenumii)}
\newcommand{\id}[1]{\texttt{#1}}
\newcommand{\kw}[1]{\texttt{\textbf{#1}}}
%\newcommand{sgn}{{\operatorname{sgn}}}

\usepackage{enumitem}
\setenumerate[1]{itemsep=0pt}

\begin{document}

\title{一个Linux上用python直接处理csv文件的方法}
\author{\href{mailto:xtwxy@hotmail.com}{汪兴元}}
\date{2015 年 7 月 21 日}
\maketitle
\abstract{
我们常常需要处理体积很大的csv数据或日志文件。一般来讲，
导入RDBMS来处理是常用的一种办法，但此办法也不完美，尤其是SQL这种描述型的语言。
在表达算法的时候不如一般的程序设计语言的表达力强，某些应用场合虽然能够实现，
但是难度太高，优化不易；批量处理大数据，通过在RDBMS上运行SQL效率并不高。

另一种办法就是使用Hadoop或Spark，
或导入NoSQL中，再使用MapReduce。但是使用类似Hadoop之类的工具又可能太重，
数据量不够运作一个Hadoop集群，还得承受其代价。

本文界绍了直接处理csv文件的一套办法，作为另一个数据处理的选项，示例代码用python，
运行的操作系统为Linux，源程序及csv数据文件的编码采用utf8。
在从原始数据到最终的目标结果，需要组合本文提到的各种算法，才能达到目的。
通过管道-过滤器连接每一个算法，能够将整个处理算法分而治之，同时得到比较好的性能。

本文假设要处理的整个数据集无法一次装入机器的内存。
}

\tableofcontents

\section{校验数据}
csv文件可能存在错误。在正常情况下，csv文件不应存在错误。但是写入csv的过程并非
事务型的，不象RDBMS那样有很好的ACID保证，故磁盘耗尽，进程异常退出、挂起等因素，
直接导致csv文件的格式并非预期。

检查数据没有统一的方法，要视数据自身的特点来做检查。一般是检查一些数据正确
所需的必要条件，但必要并不一定充分。一般的检查方法有：
\begin{enumerate}
  \item 检查列数。如果列数不等于预期值，可以确定此行数据错误。
  \item 检查每列数据的格式，比如，数字，日期，或其它满足指定格式的文本串。
    可以偿试将其解析为对应的类型，看能否成功；或用正则表达式验证。
  \item 校验字段的值。前两项都对的情况下，可以检查数据值的取值范围。
    比如健在的人的年龄不能是负数，也不能是数百以上；历史数据中的记录时间不
    可能超过当前的日历时钟。 
\end{enumerate}
\lstinputlisting{python-src/validate_history_ai.py}
如果数据是已经通过有严格校验的系统中生成或导出的，原始数据本身无误，
一般做以上三项检查可以查出
我们能见到的全部错误。但是如果数据是人工填写或是有故障的机器生成的，
这三项检查仍不能确保检查出全部错误。

对于错误的数据该如何处理，要视情况而定。有的业务场景，例如Web服务器的
access log，价值不高；又如水温传感器输入的每分钟
一次的历史数据，可以丢弃，之后使用插补法补缺；有的场景是不可以这样做的，
如交易记录，需要重新导出此段数据或人工处理。样例代码中我们采用了丢弃的方法处理。

以上代码没有直接打开csv文件，而是从标准输入中读取文件，处理完毕之后再写到标准
输出。这样的好处是便于通过管道过滤器连接多个处理进程，避免过高的耦合度。
本文所有的处理程序都使用管道过滤器连接，不再赘述。

程序中对校验2.和3.未实现，可以练习下。对于机器导出的数据，
做完1.可以去掉大部分错误。如果不打算实现2.和3.

可以将列数从命令行上读取，成为更通用的子程序。这个作为练习。

\section{选择列}
原始数据中有可能只有部分列是所要关心的，其它的与要解决的问题无关，可以丢弃。
因此要选择列，类似SQL语句中的\id{SELECT}所要办的事情。
样例程序如下所示，这里我们只对第1，3，4，5列感兴趣。
\lstinputlisting{python-src/select_history_ai.py}

可以将列下标从命令行输入，这样这个程序就成为一个通用的选择程序，而不是仅用于
示例的场景。这个作为练习。


\section{排序}
我们对数据做去重、转置、分组、聚集，都需要先对数据排序。因此排序是非常重要的
功能。不可或缺。

大数据的排序由于无法直接一次装入内存，故必须使用外部排序。
如果能将数据切成能用内部排序的小块，排好序之后，再合并，
那么我们就可以完成对大数据的排序。

我们不打算从头实现一个大数据排序工具。利用Linux的工具\id{sort}来做排序。
先将csv文件拆分。可以在导出或生成csv的时候，每达到某一固定尺寸就rotate一下，
当前文件改名，再创建一个新文件当作当前文件。

假设文件已经切成能用内存装下的多个小文件。注意，使用切割工具切文件，
必须从整行数据的位置断开，否则，切口处的那条数据被切坏了。

对每一个文件，执行\id{sort}。下面的两段代码是在Linux的终端上输入的命令，“\id{\$}”表示
命令提示符。示例中有两个文件，这里只写了第一个文件的排序，另一个省略。
\begin{lstlisting}[language=sh]
$ cat data/att-rec-utf8_part-1.txt \ 
  | python python-src/select_att_rec.py \ 
  | sort --field-separator=, --ignore-leading-blanks --stable \ 
  --key=1.2n,2 --key=2,3 >part-1.csv
\end{lstlisting}
此命令先用\id{select\_att\_rec.py}选择所要的列，再对选择的结果排序，之后将结果重定向
到文件\id{part-1.csv}。所有小文件都排序好之后，再对排序后的文件做merge:
\begin{lstlisting}[language=sh]
$ sort --field-separator=, --ignore-leading-blanks --stable \
  --key=1.2n,2 --key=2,3 -m part-*.csv > merge-sorted.csv
\end{lstlisting}
注意在做merge之前，必须先对每个文件排好序；merge时，使用的关键字、分隔符等参
数必须与\id{sort}的时候所用的参数完全一致。

merge完之后，得到一个已经排好序的大文件。

\section{转换ID字段} \label{sec-transform-id}
一般在集成两个系统的数据的时候，需要将两个系统的数据合并起来。但是两个系统的
数据ID一般是相互独立分配的，因此，如果ID都是同一种类型，比如整数，几乎100\%
会发生冲突，即不相同的两个对象，其ID相同。即使用字符串之类的做ID，也难以不发
生冲突。因此必须要建立两个系统间的对象的对应关系。

例如，商品销售网站和库管理系统如果是两套独立的系统，分别由不同的团队开发，
比如库存管理系统是外购的，商品销售网站是自建的，如果要将两套系统的数据整
起来合做分析，又没有办法修改系统达到统一，就需要做ID转换。

下面的程序从标准输入读入历史数据，从命令行读取基础数据文件名，加载基础数据到
字典中，利用基础数据中的$R(\mathrm{ID}_{src}, \mathrm{ID}_{dest})$关系，做了一个映射。逐行将
原ID映射到目标ID的方法替换掉ID，实现ID转换的目的。
\lstinputlisting{python-src/transform_data_id.py}

一般来讲，大多数系统中的某一基础数据对象的ID是可以一次全部装入内存的。比如超市
的商品ID，虽然可能有数十万条，但是对于现代计算机的内存来讲并不算多。

对于历史记录，其身的ID，或Primary Key的数量大，其总体积有可能单台机器的内存装
不下。不过一般情况下，需要做转换的必要性往往不大，
只对其通过Foreign Key引用的基础数据对象的ID有转换的价值。如果实在需要转换，
使用\ref{sec-inner-join}提到的内连接方法来做大表的连接，再选择所要的字段，包括新ID，
不选原ID字段即可。

这个方法除了可以用于转换ID的场合，还可以用于更换名称。比如，我要将公司员工的
行为的分析结果公布，但是不想让人知道员工的真实姓名或工号；或者谁购买了伟哥，
将要商品名换掉。

样例程序中的ID列号、多少列都是hardcode进代码的。可以将关系搞成动态的，从命令行
读入。设$C_{i_1}, \cdots, C_{i_m}$是从要转换的数据中选出来作为原ID的列，
$C_{j_1}, \cdots, C_{j_n}$是要转成的目标ID列，$m, n \geqslant 1$。
则转码可以视为从
\begin{equation} \label{transform-id}
\begin{aligned} 
transform: R_{src} &\to R_{dest} \\
\mathrm{ID}_{src}(C_{i_1}, \cdots, C_{i_m}) &\mapsto \mathrm{ID}_{dest}(C_{j_1}, \cdots, C_{j_n}) 
\end{aligned} 
\end{equation}
只需要在命令行上输入源ID的列号清单和目标ID的列号清单，
在程序中，用迭代清单代替硬编码的取列语句即可。将hardcode转为动态，作为练习。

\section{去重复值，整行\id{distinct()}}
如果在内存中，用\id{set}来装数据，自动去重。但是我们的数据量大，不能装入内存，
因此直接用\id{set}不行。如果对每个输入值，都去扫描一遍记录，如果共有$n$个记录，
则对第$1$个记录，要扫描第$2, \cdots, n$共$n-1$个记录；
对第$2$个记录，要扫描第$3, \cdots, n$共$n-2$个记录，
对第$n-1$个记录要扫描第$n$个记录共$1$个记录。于是
一共要扫描文件$n-1$次，转储到文件$n-1$次，其中最后一次是结果，
中间$n-2$次是临时文件。这个性能相当差。共扫描记录的个次是
\begin{equation} \label{scan-identical}
\begin{aligned}
(n-1) + \cdots + 1 = \frac{1}{2} n (n-1)\text{。} 
\end{aligned} 
\end{equation}
csv文件每行的长度不固定，因此，定位记录的位置很难，往往扫描的记录数是$n^2$。

如果将数据先排序，则相同的两条记录会相邻，成为一组，于是，我们只需要比较相邻
的两条数据是否相同，如果相同就丢弃一条，如果不同就输出这一条。示例代码如下，
假设数据事先已经排好序。
\lstinputlisting{python-src/remove_duplicate.py}

\section{连接}
数据仅代表同一个表或关系的csv是不够的。两个表csv连接跟写SQL查询一样常见。
如果是历史记录表跟基础数据表的连接，一般基础数据很小，历史数据大，所以可以
采用~\ref{sec-transform-id}所述的方法。即把历史表的Key作为\id{dict}的Key，
基础表的Key作为\id{dict}的Value，逐行转换。

如果是内连接，则对于每行历史记录，必须找到对应的基础表中的记录才算连接上，
否则要丢弃；而对于左连接，找不到对应的记录，则用空值填充。

对于右连接，和全外连接，由于历史数据很大，不能直接装入内存，这办法不适用。

接下来描述不将表装入内存的连接方法。

\subsection{内连接} \label{sec-inner-join}
先将两个表的csv按连接的关键字排序，升序或降序都行，但必须都是升序或都是降序。

再逐行连接。不妨假设都按升序排好了序。设两个表分别为$R_1$，$R_2$，
当前从这两张表取的数据行的行$r_1$，$r_2$。

\begin{enumerate}
  \item 先从$R_1$中取第一行为$r_1$，取其连接的Key，设为$K_1$，
  取$R_2$中的第一行为$r_2$，取其Key，$K_2$，进行
比较。如果$K_1 < K_2$，由于$R_2$为升序，$r_2$及$R_2$余下的行的Key均不小于$K_2$，
因此，$r_1$与$r_2$及$R_2$余下任一行都连接不上。于是再取$R_1$中的
下一行为$r_1$。

  \item 如果$K_1 > K_2$，由于$R_1$升序，$r_1$及$R_1$余下的行的Key均不小于$K_1$，
  因此，$r_2$与$r_1$及$R_1$余下任一行都连接不上。于是再取$R_2$中的下一行为$r_2$。

  \item 如果$K_1 = K_2$，则连接成功，输出连接后的行。再取$R_2$的下一行为$r_2$。
重复以上步骤，直至所有的数据处理完毕。
\end{enumerate}
此算法的python实现如下。
\lstinputlisting{python-src/inner_join.py}

此算法利用数据集的顺序，避免了重复扫描csv，只需one pass即可完成连接；
缺点是必须先排序。

可以考虑将程序写成通用的，在命令行上指定连接谓词。这个可以作为练习。
\subsection{左连接}
\subsection{右连接}
\subsection{全外连接}
\section{过滤数据}
\subsection{谓词及表示方法}
\subsection{谓词的连接关系}
\subsubsection{AND}
\subsubsection{OR}
\subsubsection{优先级}
\section{聚集}
\subsection{\kw{group by}}
\subsection{\kw{having}}
\subsection{\id{max()}, \id{min()}, \id{avg()}}
\section{补缺失值}

\section{转置}

\end{document}
